# Training configuration

# Optimization
optimizer: "adam"
learning_rate: 0.001
weight_decay: 0.0001
batch_size: 32

# Training
epochs: 100
early_stopping:
  enabled: true
  patience: 15
  min_delta: 0.001

# Learning rate scheduling
lr_scheduler:
  enabled: true
  type: "reduce_on_plateau"
  factor: 0.5
  patience: 5

# Checkpointing
checkpoint:
  enabled: true
  monitor: "val_loss"
  mode: "min"
  save_best_only: true

# Device
device: "cuda"  # or "cpu"
