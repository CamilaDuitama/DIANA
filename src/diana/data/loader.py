"""
Data Loaders for K-mer Matrices and Metadata
=============================================

Provides fast, polars-based loading of k-mer matrices and associated metadata
for machine learning workflows.

DEPENDENCIES:
-------------
Python packages:
  - polars (fast CSV/matrix reading, preferred over pandas)
  - numpy (array operations)
  - pandas (sklearn compatibility when needed)
  - scipy (sparse matrices, optional)

Input file formats:
  - K-mer matrix (.pa.mat, .abundance.mat): Space-separated matrix files
    * Column 0: Sample ID (e.g., ERR001, SRR002)
    * Columns 1+: Feature values (k-mer presence/absence or abundances)
    * Generated by: muset tools or scripts/data_prep/05_extract_and_split_matrices.py
  
  - Metadata (.tsv): Tab-separated metadata files
    * Must contain 'Run_accession' column matching sample IDs in matrix
    * Additional columns: Classification targets, sample information
    * Generated by: scripts/data_prep/01_create_splits.py

CLASSES:
--------
MatrixLoader: Fast loading and alignment of k-mer matrices with metadata

USAGE:
------
# Load matrix only:
loader = MatrixLoader("data/splits/train_matrix.pa.mat")
features, sample_ids, _ = loader.load()

# Load matrix with aligned metadata:
loader = MatrixLoader("data/splits/train_matrix.pa.mat")
features, metadata = loader.load_with_metadata(
    metadata_path="data/splits/train_metadata.tsv",
    align_to_matrix=True
)

Used by:
  - scripts/training/07_train_multitask_single_fold.py
  - All training and evaluation scripts requiring matrix data
"""

import polars as pl
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, List, Optional, Dict
import scipy.sparse as sp
import logging

logger = logging.getLogger(__name__)


class MatrixLoader:
    """
    Load k-mer/unitig matrices in .mat format (space-separated with IDs).
    
    Matrix format: Each row is a FEATURE (k-mer/unitig), NOT a sample
        Column 0: Feature ID (unitig_id, k-mer sequence, etc.)
        Columns 1+: Sample values (presence/absence, fractions, counts)
    
    Example file format:
        496659 0.0 1.0 0.0 1.0 ...  (feature ID 496659, values for each sample)
        545130 1.0 0.0 1.0 0.0 ...  (feature ID 545130, values for each sample)
        ...
    
    Returns: Matrix is TRANSPOSED to (n_samples, n_features) for ML compatibility
    
    Features:
      - Fast polars-based CSV reading (10x faster than numpy.loadtxt)
      - Auto-transposes to (samples × features) for sklearn/torch
      - Sample ID extraction from file-of-files (FOF) when available
      - Memory-efficient processing
    
    Used by: 
      - scripts/training/07_train_multitask_single_fold.py
      - scripts/data_prep/06_analyze_unitigs.py
      - Any script requiring k-mer/unitig matrix loading
    """
    
    def __init__(self, matrix_path: Path):
        """
        Initialize matrix loader.
        
        Args:
            matrix_path: Path to .pa.mat, .frac.mat, or .abundance.mat file
        """
        self.matrix_path = Path(matrix_path)
        
    def load(self, return_pandas: bool = False) -> Tuple[np.ndarray, np.ndarray, Optional[pd.DataFrame]]:
        """
        Load k-mer/unitig matrix using polars for fast I/O.
        
        Matrix file has features as rows. This method transposes to (samples × features).
        
        Args:
            return_pandas: If True, return sample IDs as pandas DataFrame for sklearn compatibility
            
        Returns:
            Tuple of (features_matrix, sample_ids, sample_ids_df)
            - features: np.ndarray of shape (n_samples, n_features)
            - sample_ids: np.ndarray of sample IDs (extracted from directory or inferred)
            - sample_ids_df: Optional pandas DataFrame with 'Run_accession' column
        """
        logger.info(f"Loading matrix from {self.matrix_path}")
        
        # Load with polars (fast!)
        df = pl.read_csv(
            self.matrix_path,
            separator=' ',
            has_header=False,
            infer_schema_length=0  # Load as strings first, then convert
        )
        
        # Extract feature IDs (first column) - not used but shows we loaded correctly
        feature_ids = df[:, 0].to_numpy()
        
        # Extract data (remaining columns) as float32
        # Shape: (n_features, n_samples)
        data = df[:, 1:].to_numpy().astype(np.float32)
        
        logger.info(f"Loaded {data.shape[0]} features × {data.shape[1]} samples (before transpose)")
        
        # Transpose to (n_samples, n_features) for ML compatibility
        features = data.T
        logger.info(f"Transposed to {features.shape[0]} samples × {features.shape[1]} features")
        
        # Try to get sample IDs from kmtricks.fof in parent directory
        sample_ids = self._get_sample_ids(expected_count=features.shape[0])
        
        # Optionally return pandas DataFrame for sklearn compatibility
        sample_ids_df = None
        if return_pandas:
            sample_ids_df = pd.DataFrame({'Run_accession': sample_ids})
        
        return features, sample_ids, sample_ids_df
    
    def _get_sample_ids(self, expected_count: int) -> np.ndarray:
        """
        Extract sample IDs from kmtricks.fof file or generate placeholder IDs.
        
        Args:
            expected_count: Number of samples expected (for validation)
            
        Returns:
            Array of sample IDs
        """
        # Look for kmtricks.fof in matrix directory structure
        matrix_dir = self.matrix_path.parent
        fof_candidates = [
            matrix_dir / "kmer_matrix" / "kmtricks.fof",
            matrix_dir.parent / "kmer_matrix" / "kmtricks.fof",
            matrix_dir / "kmtricks.fof"
        ]
        
        for fof_path in fof_candidates:
            if fof_path.exists():
                logger.info(f"Reading sample IDs from {fof_path}")
                sample_ids = []
                with open(fof_path, 'r') as f:
                    for line in f:
                        # FOF format: sample_id : /path/to/file
                        sample_id = line.strip().split()[0]
                        sample_ids.append(sample_id)
                
                if len(sample_ids) == expected_count:
                    logger.info(f"Sample IDs: {sample_ids[:3]}...")
                    return np.array(sample_ids)
                else:
                    logger.warning(f"FOF has {len(sample_ids)} IDs but matrix has {expected_count} samples")
        
        # Fallback: generate placeholder IDs
        logger.warning(f"kmtricks.fof not found, generating placeholder sample IDs")
        sample_ids = np.array([f"sample_{i:03d}" for i in range(expected_count)])
        logger.info(f"Sample IDs: {sample_ids[:3]}...")
        return sample_ids
    
    def load_with_metadata(
        self,
        metadata_path: Path,
        align_to_matrix: bool = True
    ) -> Tuple[np.ndarray, pl.DataFrame]:
        """
        Load matrix and align metadata to match sample order.
        
        Args:
            metadata_path: Path to metadata TSV
            align_to_matrix: If True, reorder metadata to match matrix sample order
            
        Returns:
            Tuple of (features_matrix, metadata_df)
        """
        # Load matrix
        features, sample_ids, _ = self.load(return_pandas=False)
        
        # Load metadata with polars
        logger.info(f"Loading metadata from {metadata_path}")
        metadata = pl.read_csv(metadata_path, separator='\t')
        logger.info(f"Metadata: {metadata.height} rows × {metadata.width} columns")
        
        if align_to_matrix:
            # Filter to samples present in matrix
            metadata = metadata.filter(
                pl.col('Run_accession').is_in(sample_ids)
            )
            
            # Reorder to match matrix sample order
            sample_id_to_idx = {sid: i for i, sid in enumerate(sample_ids)}
            metadata = metadata.with_columns(
                pl.col('Run_accession').map_elements(
                    lambda x: sample_id_to_idx.get(x, -1),
                    return_dtype=pl.Int64
                ).alias('_order')
            ).sort('_order').drop('_order')
            
            logger.info("Metadata aligned to matrix sample order")
        
        # Verify alignment
        if metadata.height != features.shape[0]:
            raise ValueError(
                f"Metadata rows ({metadata.height}) != matrix samples ({features.shape[0]})"
            )
        
        return features, metadata


class MetadataLoader:
    """
    Load and process metadata for classification.
    
    Handles loading of TSV-formatted metadata files and basic preprocessing.
    Used by all data analysis and training scripts.
    """
    
    def __init__(self, metadata_path: Path):
        """
        Initialize metadata loader.
        
        Args:
            metadata_path: Path to metadata TSV file
        """
        self.metadata_path = Path(metadata_path)
        self.df = None
        
    def load(self) -> pl.DataFrame:
        """Load metadata from TSV."""
        self.df = pl.read_csv(
            self.metadata_path,
            separator="\t",
            infer_schema_length=0
        )
        return self.df
        
    def get_labels(self, sample_ids: List[str], 
                   target: str) -> np.ndarray:
        """
        Get labels for specified samples and target.
        
        Args:
            sample_ids: List of sample IDs
            target: Target column name (sample_type, community_type, etc.)
            
        Returns:
            Array of labels
        """
        if self.df is None:
            self.load()
            
        filtered = self.df.filter(
            pl.col("Run_accession").is_in(sample_ids)
        )
        return filtered[target].to_numpy()
        
    def get_all_labels(self, sample_ids: List[str]) -> Dict[str, np.ndarray]:
        """
        Get all target labels for specified samples.
        
        Args:
            sample_ids: List of sample IDs
            
        Returns:
            Dictionary mapping target names to label arrays
        """
        targets = ["sample_type", "community_type", "sample_host", "material"]
        return {
            target: self.get_labels(sample_ids, target)
            for target in targets
        }
