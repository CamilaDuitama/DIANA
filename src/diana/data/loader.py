"""
Data Loaders for K-mer Matrices and Metadata
=============================================

Provides fast, polars-based loading of k-mer matrices and associated metadata
for machine learning workflows.

DEPENDENCIES:
-------------
Python packages:
  - polars (fast CSV/matrix reading, preferred over pandas)
  - numpy (array operations)
  - pandas (sklearn compatibility when needed)
  - scipy (sparse matrices, optional)

Input file formats:
  - K-mer matrix (.pa.mat, .abundance.mat): Space-separated matrix files
    * Column 0: Sample ID (e.g., ERR001, SRR002)
    * Columns 1+: Feature values (k-mer presence/absence or abundances)
    * Generated by: muset tools or scripts/data_prep/05_extract_and_split_matrices.py
  
  - Metadata (.tsv): Tab-separated metadata files
    * Must contain 'Run_accession' column matching sample IDs in matrix
    * Additional columns: Classification targets, sample information
    * Generated by: scripts/data_prep/01_create_splits.py

CLASSES:
--------
MatrixLoader: Fast loading and alignment of k-mer matrices with metadata

USAGE:
------
# Load matrix only:
loader = MatrixLoader("data/splits/train_matrix.pa.mat")
features, sample_ids, _ = loader.load()

# Load matrix with aligned metadata:
loader = MatrixLoader("data/splits/train_matrix.pa.mat")
features, metadata = loader.load_with_metadata(
    metadata_path="data/splits/train_metadata.tsv",
    align_to_matrix=True
)

Used by:
  - scripts/training/07_train_multitask_single_fold.py
  - All training and evaluation scripts requiring matrix data
"""

import polars as pl
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, List, Optional, Dict
import scipy.sparse as sp
import logging

logger = logging.getLogger(__name__)


class MatrixLoader:
    """
    Load k-mer matrices in .mat format (space-separated with sample IDs).
    
    Matrix format: Each row is a sample
        Column 0: Sample ID (e.g., ERR001, SRR002)
        Columns 1+: Feature values (k-mer presence/absence or counts)
    
    Example:
        ERR001 0.0 1.0 0.0 1.0 ...
        ERR002 1.0 0.0 1.0 0.0 ...
        ...
    
    Features:
      - Fast polars-based CSV reading (10x faster than numpy.loadtxt)
      - Automatic alignment with metadata
      - Sample ID extraction and validation
      - Memory-efficient processing
    
    Used by: 
      - scripts/training/07_train_multitask_single_fold.py
      - Any script requiring k-mer matrix loading
    """
    
    def __init__(self, matrix_path: Path):
        """
        Initialize matrix loader.
        
        Args:
            matrix_path: Path to .pa.mat or .abundance.mat file
        """
        self.matrix_path = Path(matrix_path)
        
    def load(self, return_pandas: bool = False) -> Tuple[np.ndarray, np.ndarray, Optional[pd.DataFrame]]:
        """
        Load k-mer matrix using polars for fast I/O.
        
        Args:
            return_pandas: If True, return sample IDs as pandas DataFrame for sklearn compatibility
            
        Returns:
            Tuple of (features_matrix, sample_ids, sample_ids_df)
            - features: np.ndarray of shape (n_samples, n_features)
            - sample_ids: np.ndarray of sample IDs
            - sample_ids_df: Optional pandas DataFrame with 'Run_accession' column
        """
        logger.info(f"Loading matrix from {self.matrix_path}")
        
        # Load with polars (fast!)
        df = pl.read_csv(
            self.matrix_path,
            separator=' ',
            has_header=False,
            infer_schema_length=0  # Load as strings first, then convert
        )
        
        # Extract sample IDs (first column)
        sample_ids = df[:, 0].to_numpy()
        
        # Extract features (remaining columns) as float32
        features = df[:, 1:].to_numpy().astype(np.float32)
        
        logger.info(f"Loaded {features.shape[0]} samples × {features.shape[1]} features")
        logger.info(f"Sample IDs: {sample_ids[:3]}...")
        
        # Optionally return pandas DataFrame for sklearn compatibility
        sample_ids_df = None
        if return_pandas:
            sample_ids_df = pd.DataFrame({'Run_accession': sample_ids})
        
        return features, sample_ids, sample_ids_df
    
    def load_with_metadata(
        self,
        metadata_path: Path,
        align_to_matrix: bool = True
    ) -> Tuple[np.ndarray, pl.DataFrame]:
        """
        Load matrix and align metadata to match sample order.
        
        Args:
            metadata_path: Path to metadata TSV
            align_to_matrix: If True, reorder metadata to match matrix sample order
            
        Returns:
            Tuple of (features_matrix, metadata_df)
        """
        # Load matrix
        features, sample_ids, _ = self.load(return_pandas=False)
        
        # Load metadata with polars
        logger.info(f"Loading metadata from {metadata_path}")
        metadata = pl.read_csv(metadata_path, separator='\t')
        logger.info(f"Metadata: {metadata.height} rows × {metadata.width} columns")
        
        if align_to_matrix:
            # Filter to samples present in matrix
            metadata = metadata.filter(
                pl.col('Run_accession').is_in(sample_ids)
            )
            
            # Reorder to match matrix sample order
            sample_id_to_idx = {sid: i for i, sid in enumerate(sample_ids)}
            metadata = metadata.with_columns(
                pl.col('Run_accession').map_elements(
                    lambda x: sample_id_to_idx.get(x, -1),
                    return_dtype=pl.Int64
                ).alias('_order')
            ).sort('_order').drop('_order')
            
            logger.info("Metadata aligned to matrix sample order")
        
        # Verify alignment
        if metadata.height != features.shape[0]:
            raise ValueError(
                f"Metadata rows ({metadata.height}) != matrix samples ({features.shape[0]})"
            )
        
        return features, metadata


class MetadataLoader:
    """
    Load and process metadata for classification.
    
    Handles loading of TSV-formatted metadata files and basic preprocessing.
    Used by all data analysis and training scripts.
    """
    
    def __init__(self, metadata_path: Path):
        """
        Initialize metadata loader.
        
        Args:
            metadata_path: Path to metadata TSV file
        """
        self.metadata_path = Path(metadata_path)
        self.df = None
        
    def load(self) -> pl.DataFrame:
        """Load metadata from TSV."""
        self.df = pl.read_csv(
            self.metadata_path,
            separator="\t",
            infer_schema_length=0
        )
        return self.df
        
    def get_labels(self, sample_ids: List[str], 
                   target: str) -> np.ndarray:
        """
        Get labels for specified samples and target.
        
        Args:
            sample_ids: List of sample IDs
            target: Target column name (sample_type, community_type, etc.)
            
        Returns:
            Array of labels
        """
        if self.df is None:
            self.load()
            
        filtered = self.df.filter(
            pl.col("Run_accession").is_in(sample_ids)
        )
        return filtered[target].to_numpy()
        
    def get_all_labels(self, sample_ids: List[str]) -> Dict[str, np.ndarray]:
        """
        Get all target labels for specified samples.
        
        Args:
            sample_ids: List of sample IDs
            
        Returns:
            Dictionary mapping target names to label arrays
        """
        targets = ["sample_type", "community_type", "sample_host", "material"]
        return {
            target: self.get_labels(sample_ids, target)
            for target in targets
        }
