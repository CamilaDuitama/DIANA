#!/usr/bin/env python3
"""
Collect and Aggregate Multi-Task Cross-Validation Results
==========================================================

Aggregates results across all folds from hyperparameter optimization,
computes mean±std metrics for each task, and identifies the best 
hyperparameters to use for final model training.

DEPENDENCIES:
-------------
Python packages:
  - numpy, pandas (data manipulation)
  - json (reading result files)

Input files:
  - Cross-validation results from each fold (JSON files)
    Generated by: scripts/training/07_train_multitask_single_fold.py
    Location: results/multitask_gpu/fold_*/multitask_fold_*_results_*.json

OUTPUT:
-------
Prints to stdout:
  - Per-task metrics (mean ± std) across all folds
  - Best hyperparameters (most common or best performing)
  - Overall summary statistics

USAGE:
------
# After completing all 5 folds of training:
python scripts/evaluation/collect_multitask_results.py \\
    --results-dir results/multitask_gpu

# With output file:
python scripts/evaluation/collect_multitask_results.py \\
    --results-dir results/multitask_gpu \\
    --output results/multitask_gpu/aggregated_results.json

WORKFLOW:
---------
1. Scan results directory for fold_* subdirectories
2. Load most recent results JSON from each fold
3. Aggregate metrics across folds (mean, std, min, max)
4. Identify best hyperparameters based on consistency or performance
5. Print summary and optionally save to JSON
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
import numpy as np
import pandas as pd
from collections import Counter


def format_hyperparameters(flat_params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convert flat Optuna hyperparameters to nested structure for final training.
    
    Args:
        flat_params: Flat dictionary with keys like n_layers, hidden_dim_0, etc.
        
    Returns:
        Nested dictionary with model_params, trainer_params, and batch_size
    """
    # Extract number of layers (rounded since it's averaged)
    n_layers = int(round(flat_params['n_layers']))
    
    # Reconstruct hidden_dims list from hidden_dim_{i} keys
    # Only use dimensions that exist (some folds may have had different n_layers)
    hidden_dims = []
    for i in range(n_layers):
        key = f'hidden_dim_{i}'
        if key in flat_params:
            hidden_dims.append(int(round(flat_params[key])))
        else:
            # If this dimension wasn't present in all folds, skip or use default
            # This shouldn't happen if averaging worked correctly
            print(f"Warning: {key} not found, using 256 as default")
            hidden_dims.append(256)
    
    # Extract task weights
    task_weights = {
        'sample_type': float(flat_params['task_weight_sample_type']),
        'community_type': float(flat_params['task_weight_community']),
        'sample_host': float(flat_params['task_weight_host']),
        'material': float(flat_params['task_weight_material'])
    }
    
    # Build nested structure
    formatted = {
        'model_params': {
            'hidden_dims': hidden_dims,
            'dropout': float(flat_params['dropout']),
            'activation': flat_params['activation'],
            'use_batch_norm': bool(flat_params['use_batch_norm'])
        },
        'trainer_params': {
            'learning_rate': float(flat_params['learning_rate']),
            'weight_decay': float(flat_params.get('weight_decay', 0.0)),
            'task_weights': task_weights
        },
        'batch_size': int(round(flat_params['batch_size']))
    }
    
    return formatted


def collect_fold_results(results_dir: Path) -> List[Dict[str, Any]]:
    """
    Collect results from all folds.
    
    Args:
        results_dir: Directory containing fold results
        
    Returns:
        List of results dictionaries
    """
    results = []
    
    for fold_dir in sorted(results_dir.glob("fold_*")):
        # Find the most recent results file
        result_files = list(fold_dir.glob("multitask_fold_*_results_*.json"))
        
        if not result_files:
            continue
        
        # Get most recent
        result_file = max(result_files, key=lambda p: p.stat().st_mtime)
        
        with open(result_file, 'r') as f:
            fold_results = json.load(f)
            results.append(fold_results)
    
    return results


def aggregate_metrics(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Aggregate metrics across folds.
    
    Args:
        results: List of fold results
        
    Returns:
        DataFrame with aggregated metrics
    """
    all_metrics = []
    
    for fold_result in results:
        fold_id = fold_result['fold_id']
        test_metrics = fold_result['test_metrics']
        
        for task, metrics in test_metrics.items():
            for metric_name, value in metrics.items():
                all_metrics.append({
                    'fold': fold_id,
                    'task': task,
                    'metric': metric_name,
                    'value': value
                })
    
    df = pd.DataFrame(all_metrics)
    
    # Compute mean ± std
    summary = df.groupby(['task', 'metric'])['value'].agg(['mean', 'std']).reset_index()
    summary['mean_std'] = summary.apply(lambda x: f"{x['mean']:.4f} ± {x['std']:.4f}", axis=1)
    
    return summary


def get_best_hyperparameters(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Average hyperparameters across all folds for robust final configuration.
    
    P2: Instead of picking the single best fold, we compute mean/mode across all folds.
    - Numeric hyperparameters: mean across folds
    - Categorical hyperparameters: mode (most common value)
    
    Args:
        results: List of fold results
        
    Returns:
        Averaged hyperparameters dictionary
    """
    # Collect all hyperparameters from all folds
    all_params = [fold_result['best_params'] for fold_result in results]
    
    # Get union of all parameter keys (some folds may have different n_layers)
    all_keys = set()
    for params in all_params:
        all_keys.update(params.keys())
    
    # Aggregate each parameter
    aggregated_params = {}
    
    for key in all_keys:
        # Only consider folds that have this key
        values = [params[key] for params in all_params if key in params]
        
        if not values:
            continue
            
        # Check if numeric or categorical
        if isinstance(values[0], (int, float, np.integer, np.floating)):
            # Numeric: compute mean
            aggregated_params[key] = float(np.mean(values))
        else:
            # Categorical: use mode (most common)
            counter = Counter(values)
            aggregated_params[key] = counter.most_common(1)[0][0]
    
    # Compute average F1 across all folds for reporting
    fold_scores = []
    for fold_result in results:
        test_metrics = fold_result['test_metrics']
        avg_f1 = np.mean([metrics['f1_weighted'] for metrics in test_metrics.values()])
        fold_scores.append((fold_result['fold_id'], avg_f1))
    
    overall_avg_f1 = np.mean([score for _, score in fold_scores])
    print(f"\nAveraged across {len(results)} folds (Overall Avg F1: {overall_avg_f1:.4f})")
    
    return aggregated_params


def print_summary(summary_df: pd.DataFrame, best_params: Dict[str, Any]):
    """Print summary of results."""
    
    print("\n" + "="*80)
    print("MULTI-TASK MLP CROSS-VALIDATION RESULTS")
    print("="*80)
    
    print("\n--- Per-Task Performance (Mean ± Std across folds) ---\n")
    
    for task in summary_df['task'].unique():
        print(f"\n{task.upper()}:")
        task_df = summary_df[summary_df['task'] == task]
        for _, row in task_df.iterrows():
            print(f"  {row['metric']:20s}: {row['mean_std']}")
    
    print("\n" + "="*80)
    print("AVERAGED HYPERPARAMETERS (for final training)")
    print("="*80 + "\n")
    
    # n_layers is averaged, so round it
    n_layers = int(round(best_params['n_layers']))
    
    print("Architecture:")
    print(f"  Avg layers: {best_params['n_layers']:.1f} (using {n_layers})")
    hidden_dims = [int(round(best_params[f'hidden_dim_{i}'])) for i in range(n_layers) if f'hidden_dim_{i}' in best_params]
    print(f"  Hidden dims: {hidden_dims}")
    print(f"  Activation: {best_params['activation']}")
    print(f"  Batch norm: {best_params['use_batch_norm']}")
    print(f"  Dropout: {best_params['dropout']:.4f}")
    
    print("\nOptimization:")
    print(f"  Learning rate: {best_params['learning_rate']:.6f}")
    print(f"  Weight decay: {best_params.get('weight_decay', 0.0):.6f}")
    print(f"  Batch size: {int(round(best_params['batch_size']))}")
    
    print("\nTask Weights:")
    print(f"  sample_type: {best_params['task_weight_sample_type']:.4f}")
    print(f"  community_type: {best_params['task_weight_community']:.4f}")
    print(f"  sample_host: {best_params['task_weight_host']:.4f}")
    print(f"  material: {best_params['task_weight_material']:.4f}")
    
    print("\n" + "="*80)


def save_best_config(best_params: Dict[str, Any], output_path: Path):
    """Save best hyperparameters for final training in formatted nested structure."""
    
    # P1: Format hyperparameters into clean nested structure
    formatted_params = format_hyperparameters(best_params)
    
    config = {
        "model_type": "MultiTaskMLP",
        "description": "Best hyperparameters from 5-fold CV (averaged across all folds)",
        "hyperparameters": formatted_params,
        "_raw_averaged_params": best_params  # Keep raw for reference
    }
    
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nFormatted config saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='Collect multi-task MLP results')
    parser.add_argument('--results-dir', type=str, default='results/multitask/hyperopt',
                       help='Directory containing fold results')
    parser.add_argument('--output', type=str, 
                       help='Output path for summary (default: {results_dir}/aggregated_results.json)')
    parser.add_argument('--save-config', action='store_true',
                       help='Save best hyperparameters config')
    
    args = parser.parse_args()
    
    results_dir = Path(args.results_dir)
    
    # Default output path based on results directory
    if args.output is None:
        output_path = results_dir / 'aggregated_results.json'
    else:
        output_path = Path(args.output)
    
    if not results_dir.exists():
        print(f"Error: Results directory not found: {results_dir}")
        return
    
    # Collect results
    results = collect_fold_results(results_dir)
    
    if not results:
        print("Error: No results found!")
        return
    
    print(f"Collected {len(results)} folds")
    
    # Aggregate metrics
    summary_df = aggregate_metrics(results)
    
    # Get best hyperparameters
    best_params = get_best_hyperparameters(results)
    
    # Print summary
    print_summary(summary_df, best_params)
    
    # Save summary
    summary_data = {
        "num_folds": len(results),
        "metrics_summary": summary_df.to_dict('records'),
        "best_hyperparameters": best_params,
        "fold_results": results
    }
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(summary_data, f, indent=2)
    
    # Save best config if requested
    if args.save_config:
        config_path = results_dir / "best_config_for_final_training.json"
        save_best_config(best_params, config_path)


if __name__ == "__main__":
    main()
