#!/usr/bin/env python3
"""
Collect and Aggregate Multi-Task Cross-Validation Results
==========================================================

Aggregates results across all folds from hyperparameter optimization,
computes mean±std metrics for each task, and identifies the best 
hyperparameters to use for final model training.

DEPENDENCIES:
-------------
Python packages:
  - numpy, pandas (data manipulation)
  - json (reading result files)

Input files:
  - Cross-validation results from each fold (JSON files)
    Generated by: scripts/training/07_train_multitask_single_fold.py
    Location: results/multitask_gpu/fold_*/multitask_fold_*_results_*.json

OUTPUT:
-------
Prints to stdout:
  - Per-task metrics (mean ± std) across all folds
  - Best hyperparameters (most common or best performing)
  - Overall summary statistics

USAGE:
------
# After completing all 5 folds of training:
python scripts/evaluation/collect_multitask_results.py \\
    --results-dir results/multitask_gpu

# With output file:
python scripts/evaluation/collect_multitask_results.py \\
    --results-dir results/multitask_gpu \\
    --output results/multitask_gpu/aggregated_results.json

WORKFLOW:
---------
1. Scan results directory for fold_* subdirectories
2. Load most recent results JSON from each fold
3. Aggregate metrics across folds (mean, std, min, max)
4. Identify best hyperparameters based on consistency or performance
5. Print summary and optionally save to JSON
"""

import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
import numpy as np
import pandas as pd

sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))


def collect_fold_results(results_dir: Path) -> List[Dict[str, Any]]:
    """
    Collect results from all folds.
    
    Args:
        results_dir: Directory containing fold results
        
    Returns:
        List of results dictionaries
    """
    results = []
    
    for fold_dir in sorted(results_dir.glob("fold_*")):
        # Find the most recent results file
        result_files = list(fold_dir.glob("multitask_fold_*_results_*.json"))
        
        if not result_files:
            print(f"Warning: No results found in {fold_dir}")
            continue
        
        # Get most recent
        result_file = max(result_files, key=lambda p: p.stat().st_mtime)
        
        with open(result_file, 'r') as f:
            fold_results = json.load(f)
            results.append(fold_results)
            print(f"Loaded {result_file.name}")
    
    return results


def aggregate_metrics(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Aggregate metrics across folds.
    
    Args:
        results: List of fold results
        
    Returns:
        DataFrame with aggregated metrics
    """
    all_metrics = []
    
    for fold_result in results:
        fold_id = fold_result['fold_id']
        test_metrics = fold_result['test_metrics']
        
        for task, metrics in test_metrics.items():
            for metric_name, value in metrics.items():
                all_metrics.append({
                    'fold': fold_id,
                    'task': task,
                    'metric': metric_name,
                    'value': value
                })
    
    df = pd.DataFrame(all_metrics)
    
    # Compute mean ± std
    summary = df.groupby(['task', 'metric'])['value'].agg(['mean', 'std']).reset_index()
    summary['mean_std'] = summary.apply(lambda x: f"{x['mean']:.4f} ± {x['std']:.4f}", axis=1)
    
    return summary


def get_best_hyperparameters(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Select best hyperparameters based on average performance.
    
    Args:
        results: List of fold results
        
    Returns:
        Best hyperparameters dictionary
    """
    # Compute average F1 weighted across all tasks for each fold
    fold_scores = []
    
    for fold_result in results:
        test_metrics = fold_result['test_metrics']
        avg_f1 = np.mean([metrics['f1_weighted'] for metrics in test_metrics.values()])
        fold_scores.append((fold_result['fold_id'], avg_f1, fold_result['best_params']))
    
    # Find best fold
    best_fold_id, best_score, best_params = max(fold_scores, key=lambda x: x[1])
    
    print(f"\nBest fold: {best_fold_id} (Avg F1: {best_score:.4f})")
    
    return best_params


def print_summary(summary_df: pd.DataFrame, best_params: Dict[str, Any]):
    """Print summary of results."""
    
    print("\n" + "="*80)
    print("MULTI-TASK MLP CROSS-VALIDATION RESULTS")
    print("="*80)
    
    print("\n--- Per-Task Performance (Mean ± Std across folds) ---\n")
    
    for task in summary_df['task'].unique():
        print(f"\n{task.upper()}:")
        task_df = summary_df[summary_df['task'] == task]
        for _, row in task_df.iterrows():
            print(f"  {row['metric']:20s}: {row['mean_std']}")
    
    print("\n" + "="*80)
    print("BEST HYPERPARAMETERS (for final training)")
    print("="*80 + "\n")
    
    print("Architecture:")
    print(f"  Layers: {best_params['n_layers']}")
    hidden_dims = [best_params[f'hidden_dim_{i}'] for i in range(best_params['n_layers'])]
    print(f"  Hidden dims: {hidden_dims}")
    print(f"  Activation: {best_params['activation']}")
    print(f"  Batch norm: {best_params['use_batch_norm']}")
    print(f"  Dropout: {best_params['dropout']:.4f}")
    
    print("\nOptimization:")
    print(f"  Learning rate: {best_params['learning_rate']:.6f}")
    print(f"  Weight decay: {best_params['weight_decay']:.6f}")
    print(f"  Batch size: {best_params['batch_size']}")
    
    print("\nTask Weights:")
    print(f"  sample_type: {best_params['task_weight_sample_type']:.4f}")
    print(f"  community_type: {best_params['task_weight_community']:.4f}")
    print(f"  sample_host: {best_params['task_weight_host']:.4f}")
    print(f"  material: {best_params['task_weight_material']:.4f}")
    
    print("\n" + "="*80)


def save_best_config(best_params: Dict[str, Any], output_path: Path):
    """Save best hyperparameters for final training."""
    
    config = {
        "model_type": "MultiTaskMLP",
        "description": "Best hyperparameters from 5-fold CV with Optuna optimization",
        "hyperparameters": best_params
    }
    
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nBest config saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='Collect multi-task MLP results')
    parser.add_argument('--results-dir', type=str, default='results/multitask/hyperopt',
                       help='Directory containing fold results')
    parser.add_argument('--output', type=str, 
                       help='Output path for summary (default: {results_dir}/aggregated_results.json)')
    parser.add_argument('--save-config', action='store_true',
                       help='Save best hyperparameters config')
    
    args = parser.parse_args()
    
    results_dir = Path(args.results_dir)
    
    # Default output path based on results directory
    if args.output is None:
        output_path = results_dir / 'aggregated_results.json'
    else:
        output_path = Path(args.output)
    
    if not results_dir.exists():
        print(f"Error: Results directory not found: {results_dir}")
        return
    
    # Collect results
    print(f"Collecting results from {results_dir}...\n")
    results = collect_fold_results(results_dir)
    
    if not results:
        print("Error: No results found!")
        return
    
    print(f"\nCollected {len(results)} folds")
    
    # Aggregate metrics
    summary_df = aggregate_metrics(results)
    
    # Get best hyperparameters
    best_params = get_best_hyperparameters(results)
    
    # Print summary
    print_summary(summary_df, best_params)
    
    # Save summary
    summary_data = {
        "num_folds": len(results),
        "metrics_summary": summary_df.to_dict('records'),
        "best_hyperparameters": best_params,
        "fold_results": results
    }
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(summary_data, f, indent=2)
    
    print(f"\nFull summary saved to: {output_path}")
    
    # Save best config if requested
    if args.save_config:
        config_path = results_dir / "best_config_for_final_training.json"
        save_best_config(best_params, config_path)


if __name__ == "__main__":
    main()
