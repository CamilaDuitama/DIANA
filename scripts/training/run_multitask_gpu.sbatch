#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --array=0-4
#SBATCH --mem=64G
#SBATCH --cpus-per-task=8
#SBATCH --gres=gpu:1
#SBATCH --qos=gpu
#SBATCH --job-name=multitask_mlp
#SBATCH --output=logs/multitask/hyperopt/hyperopt_%A_%a.out
#SBATCH --error=logs/multitask/hyperopt/hyperopt_%A_%a.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=cduitama@pasteur.fr

################################################################################
# Multi-Task MLP Hyperparameter Optimization on GPU (SLURM Array Job)
################################################################################
#
# PURPOSE:
#   Trains 5 cross-validation folds in parallel on GPU, each performing
#   Optuna-based Bayesian hyperparameter optimization for multi-task 
#   classification of ancient DNA samples.
#
# DEPENDENCIES:
#   Software:
#     - SLURM workload manager (for job submission)
#     - Mamba/Conda environment at ./env with PyTorch, Optuna, polars
#     - NVIDIA GPU with CUDA support
#   
#   Input files:
#     - K-mer matrix: Specified by FEATURES variable (default: data/splits/train_matrix.pa.mat)
#     - Metadata: Specified by METADATA variable (default: data/splits/train_metadata.tsv)
#     Generated by: scripts/data_prep/05_extract_and_split_matrices.py
#   
#   Scripts:
#     - scripts/training/07_train_multitask_single_fold.py (training logic)
#     - src/diana/models/multitask_mlp.py (model architecture)
#     - src/diana/data/loader.py (data loading)
#
# USAGE:
#   Test mode (dummy data):
#     ./scripts/training/submit_multitask.sh test
#   
#   Production mode (full dataset):
#     ./scripts/training/submit_multitask.sh prod
#   
#   Custom configuration:
#     FEATURES="path/to/matrix.mat" METADATA="path/to/metadata.tsv" \\
#     TOTAL_FOLDS=3 N_TRIALS=20 MAX_EPOCHS=100 \\
#     sbatch --array=0-2 scripts/training/run_multitask_gpu.sbatch
#
# CONFIGURATION:
#   All parameters can be overridden via environment variables.
#   Defaults are set for production runs on full dataset.
#
# OUTPUT:
#   For each fold (0 to TOTAL_FOLDS-1):
#     - SLURM logs: logs/multitask_gpu_{JOBID}_{FOLD}.out/.err
#     - Results: results/{OUTPUT_DIR}/fold_{FOLD}/
#       * best_multitask_model_fold_{FOLD}_{timestamp}.pth
#       * multitask_fold_{FOLD}_results_{timestamp}.json
#       * fold_{FOLD}_training_log_{timestamp}.txt
#
# NEXT STEPS:
#   After all folds complete, aggregate results:
#     python scripts/evaluation/collect_multitask_results.py \\
#       --results-dir results/multitask_gpu
#
################################################################################

echo "=== MULTI-TASK MLP TRAINING ==="
echo "Job ID: $SLURM_JOB_ID"
echo "Array Task ID: $SLURM_ARRAY_TASK_ID"
echo "GPU: $CUDA_VISIBLE_DEVICES"
echo "Node: $(hostname)"
echo "Date: $(date)"
echo ""

# GPU environment setup
export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Configuration with defaults (can be overridden by environment variables)
# If RUN_CONFIG is set, read from JSON config file (new CLI approach)
if [ -n "$RUN_CONFIG" ] && [ -f "$RUN_CONFIG" ]; then
    echo "Reading configuration from: $RUN_CONFIG"
    FEATURES=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['features_path'])")
    METADATA=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['metadata_path'])")
    OUTPUT_DIR=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['output_dir'])")
    TOTAL_FOLDS=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['n_folds'])")
    N_TRIALS=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['n_trials'])")
    MAX_EPOCHS=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['max_epochs'])")
    N_INNER_SPLITS=$(python -c "import json; print(json.load(open('$RUN_CONFIG'))['n_inner_splits'])")
else
    # Use environment variables or defaults (old approach)
    FEATURES=${FEATURES:-"data/splits/train_matrix.pa.mat"}
    METADATA=${METADATA:-"data/splits/train_metadata.tsv"}
    OUTPUT_DIR=${OUTPUT_DIR:-"results/multitask/hyperopt"}
    TOTAL_FOLDS=${TOTAL_FOLDS:-5}
    N_TRIALS=${N_TRIALS:-50}
    MAX_EPOCHS=${MAX_EPOCHS:-200}
    N_INNER_SPLITS=${N_INNER_SPLITS:-3}
fi

FOLD_ID=$SLURM_ARRAY_TASK_ID
LOG_DIR=${LOG_DIR:-"logs/multitask/hyperopt"}

# Create output and log directories
mkdir -p "$OUTPUT_DIR" "$LOG_DIR"

echo "Configuration:"
echo "  Fold ID: $FOLD_ID"
echo "  Total Folds: $TOTAL_FOLDS"
echo "  Optuna Trials: $N_TRIALS"
echo "  Max Epochs: $MAX_EPOCHS"
echo "  Inner CV Splits: $N_INNER_SPLITS"
echo "  Features: $FEATURES"
echo "  Metadata: $METADATA"
echo "  Output: $OUTPUT_DIR"
echo ""

# Check GPU
if command -v nvidia-smi &> /dev/null; then
    echo "GPU Information:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
    echo ""
fi

# Run training
echo "Starting hyperparameter optimization for fold $FOLD_ID..."
mamba run -p ./env python scripts/training/07_train_multitask_single_fold.py \
    --fold_id $FOLD_ID \
    --total_folds $TOTAL_FOLDS \
    --features "$FEATURES" \
    --metadata "$METADATA" \
    --output "$OUTPUT_DIR" \
    --n_trials $N_TRIALS \
    --max_epochs $MAX_EPOCHS \
    --n_inner_splits $N_INNER_SPLITS \
    --use_gpu \
    --random_seed 42

exit_code=$?

if [[ $exit_code -eq 0 ]]; then
    echo ""
    echo "=== FOLD $FOLD_ID COMPLETED SUCCESSFULLY ==="
    echo "Results saved to: $OUTPUT_DIR/fold_$FOLD_ID/"
else
    echo ""
    echo "=== FOLD $FOLD_ID FAILED ==="
    echo "Check error log: logs/multitask_gpu_${SLURM_JOB_ID}_${FOLD_ID}.err"
fi

exit $exit_code
